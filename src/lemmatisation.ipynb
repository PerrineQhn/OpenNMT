{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a054ac90-a42b-4cfb-9045-716640f19ab3",
   "metadata": {},
   "source": [
    "# Boîte à outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a68529f1-3212-4d90-a93a-301f547dbef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/perrine/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/perrine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/perrine/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FrenchLefffLemmatizer à installer si votre environnement ne le contient pas : \n",
    "# pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git\n",
    "import nltk\n",
    "\n",
    "# Télécharger les packages avec les lignes suivantes si vous ne les avez pas sur votre machine : \n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf004ed7-80ac-4223-8e24-34d1128910ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1391353-b837-4004-9866-c19422257fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatiseur anglais\n",
    "lemmatizer_en = WordNetLemmatizer()\n",
    "# Lemmatiseur français\n",
    "lemmatizer_fr = FrenchLefffLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1162911-78f7-457f-a591-2e9d14dbe7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parseur Spacy (facultatif)\n",
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123c9a0-fc63-48cf-a2ce-e81bbd9374fd",
   "metadata": {},
   "source": [
    "# Lecture des corpus prétraités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bb5b5aa-73e2-4450-aef6-b9c57406fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les chemins vers les corpus\n",
    "path_europarl = Path(\"../data/clean/Europarl_en_fr_clean/\")\n",
    "path_emea = Path(\"../data/clean/Emea_en_fr_clean/\")\n",
    "\n",
    "# Créer un dictionnaire pour enregistrer les corpus avec leurs noms de fichiers\n",
    "corpus_list = defaultdict(list)\n",
    "\n",
    "# Parcourir chaque fichier de l'Europarl\n",
    "for file in path_europarl.glob('*tok.true.clean*'):\n",
    "    with open(file, 'r') as file:\n",
    "        sents_brut = file.read()\n",
    "        # Extraire seulement le nom de fichiers comme clés \n",
    "        corpus_list[file.name.split('/')[-1]] = sents_brut.split('\\n')\n",
    "\n",
    "# Parcourir chaque fichier de l'EMEA\n",
    "for file in path_emea.glob('*tok.true.clean*'):\n",
    "    with open(file, 'r') as file:\n",
    "        sents_brut = file.read()\n",
    "        # Extraire seulement le nom de fichiers comme clés \n",
    "        corpus_list[file.name.split('/')[-1]] = sents_brut.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a5614-c2ba-4575-abec-b87450e94a31",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6592cdd-35b2-4618-a09f-de624860792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lemmatiser un texte\n",
    "def lemmatize_text(text: str, lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatisation d'un texte donné dans la langue spécifiée.\n",
    "\n",
    "    Cette fonction lemmatise un texte en utilisant NLTK pour l'anglais et Spacy ou NLTK pour le français. \n",
    "    Pour l'anglais, les parties du discours (POS tags) sont utilisées pour une lemmatisation plus précise. \n",
    "    Pour le français, la lemmatisation de base est effectuée avec lemmatizer_fr de FrenchLefffLemmatizer. \n",
    "    \n",
    "    Args:\n",
    "        text (str): Le texte à lemmatiser.\n",
    "        lang (str): La langue du texte. Peut être 'en' pour l'anglais ou 'fr' pour le français.\n",
    "\n",
    "    Returns:\n",
    "        str: Le texte lemmatisé.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si la langue spécifiée n'est ni 'en' ni 'fr'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lemmatisation de chaque mot avec son POS tag\n",
    "    lemmatized_words = []\n",
    "\n",
    "    if lang == \"en\":\n",
    "        \n",
    "        # Tokeniser le texte en mots\n",
    "        words = nltk.word_tokenize(text)\n",
    "    \n",
    "        # Obtenir les POS tags des mots\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        \n",
    "        for word, pos_tag in pos_tags:\n",
    "            # Convertir les POS tags Penn Treebank en format WordNet\n",
    "            pos_tag = pos_tag[0].lower()\n",
    "            pos_tag = pos_tag if pos_tag in ['a', 'n', 'v'] else 'n'  # Convertir les POS tags inconnus en 'n' (nom)\n",
    "    \n",
    "            \n",
    "            # Lemmatisation du mot avec son POS tag\n",
    "            lemma = lemmatizer_en.lemmatize(word, pos=pos_tag)\n",
    "            lemmatized_words.append(lemma)\n",
    "\n",
    "    elif lang == \"fr\":\n",
    "\n",
    "        # Lemmatisation avec spacy: beaucoup beaucoup plus précise mais nécessite une machine puissante \n",
    "        \n",
    "        # doc = nlp(text)\n",
    "    \n",
    "        # for token in doc:\n",
    "        #     # Lemmatisation du mot avec son POS tag\n",
    "        #     lemma = token.lemma_\n",
    "        #     lemmatized_words.append(lemma)\n",
    "        \n",
    "        \n",
    "        # Tokeniser le texte en mots\n",
    "        words = nltk.word_tokenize(text)\n",
    "            \n",
    "        # Lemmatisation du mot avec son POS tag\n",
    "        lemmatized_words = [lemmatizer_fr.lemmatize(word) for word in words]\n",
    "        \n",
    "    # Rejoindre les mots lemmatisés en une seule chaîne de caractères\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e97fbf57-6a4f-4dee-b7d3-3a40c650cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire pour stocker les phrases lemmatisées\n",
    "corpus_lemmatized = defaultdict(list)\n",
    "\n",
    "# Parcourir chaque corpus\n",
    "for k, v in corpus_list.items(): # k: nom de fichier, v: liste de phrases\n",
    "    # Créer une liste pour stocker les phrases lemmatisées\n",
    "    sents_lemmatized: list = []\n",
    "\n",
    "    # Pour les textes en anglais\n",
    "    if k.endswith('en'):\n",
    "        # Lemmatiser chaque phrase\n",
    "        for sent in v:\n",
    "            # Lemmatiser la phrase en anglais\n",
    "            sent_lemmatized = lemmatize_text(sent, 'en')\n",
    "            # Ajouter la phrase lemmatisée à la liste\n",
    "            sents_lemmatized.append(sent_lemmatized)\n",
    "        # Stocker la liste de phrases lemmatisées dans le dictionnaire\n",
    "        corpus_lemmatized[k] = sents_lemmatized\n",
    "\n",
    "    # Pour les textes en français\n",
    "    elif k.endswith('fr'):\n",
    "        # Lemmatiser chaque phrase\n",
    "        for sent in v:\n",
    "            # Lemmatiser la phrase en français\n",
    "            sent_lemmatized = lemmatize_text(sent, 'fr')\n",
    "            # Ajouter la phrase lemmatisée à la liste\n",
    "            sents_lemmatized.append(sent_lemmatized)\n",
    "        # Stocker la liste de phrases lemmatisées dans le dictionnaire\n",
    "        corpus_lemmatized[k] = sents_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc9ff6f-10ed-4b45-8b22-a2565a12367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les corpus lemmatisés\n",
    "for k, v in corpus_lemmatized.items():\n",
    "    with open(f'../data/clean/lemmatised_spacy/lemme_{k}', 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
